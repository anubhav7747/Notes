# Decision Tree
- A Decision Tree is a popular machine learning algorithm used for both classification and regression tasks. It is a tree-like model where an internal node represents a feature or attribute, the branches represent the decision rules, and each leaf node represents the outcome or class label. Decision Trees are intuitive, easy to understand, and can handle both numerical and categorical data.
- In a Decision tree, there are two nodes, which are the Decision Node and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.
- The decisions or the test are performed on the basis of features of the given dataset.
- It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.
- A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees.

![decision-tree-classification-algorithm](https://github.com/anubhav7747/Notes/assets/77168708/94503032-fcb9-48ab-87b3-860c44d4b20a)

## Why use Decision Trees?
- Decision Trees usually mimic human thinking ability while making a decision, so it is easy to understand.
- The logic behind the decision tree can be easily understood because it shows a tree-like structure.

## Key components and concepts associated with Decision Trees:
1. **Root Node:** The topmost node in the tree, representing the feature that best splits the data based on certain criteria.
2. **Internal Nodes:**
   - Intermediate nodes that represent features and decision rules.
   - Each internal node tests a specific attribute, and the data is split based on the outcome of the test.
3. **Branches:**
   - The branches or edges connecting nodes represent the possible outcomes of a decision rule.
   - For each internal node, there are branches leading to child nodes corresponding to different outcomes of the decision.

